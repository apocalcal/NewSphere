"""
íŒŒì´ì¬ ì¤‘ë³µì œê±° ì„œë¹„ìŠ¤ - 100% ì›ë³¸ ë¡œì§ êµ¬í˜„
ê¸°ì¡´ íŒŒì´ì¬ ì½”ë“œì™€ ì™„ì „íˆ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
"""

import asyncio
import time
from typing import List, Dict, Tuple, Optional, Set, Any
import structlog
from collections import defaultdict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
import torch
import re
from bs4 import BeautifulSoup
from konlpy.tag import Okt

from app.config import settings
from app.models.schemas import NewsDetail, RelatedNewsPair, DeduplicationResponse


logger = structlog.get_logger()

class DeduplicationService:
    """
    ë‰´ìŠ¤ ì¤‘ë³µì œê±° ì„œë¹„ìŠ¤ - í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    
    ì—­í• :
    - ë‰´ìŠ¤ ë°ì´í„°ì˜ ì¤‘ë³µ íƒì§€ ë° ì œê±°
    - ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (SBERT + TF-IDF)
    - ì—°ê´€ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ë° ê´€ê³„ êµ¬ì¶•
    
    ê¸°ëŠ¥:
    - ì œëª© ì „ì²˜ë¦¬: í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ë° ë¶ˆìš©ì–´ ì œê±°
    - ì¤‘ë³µ íƒì§€: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì„ê³„ê°’ ë¹„êµ
    - í´ëŸ¬ìŠ¤í„°ë§: Union-Find ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìœ ì‚¬ ë‰´ìŠ¤ ê·¸ë£¹í™”
    - ëŒ€í‘œ ì„ ì •: í´ëŸ¬ìŠ¤í„° ë‚´ ê°€ì¥ ëŒ€í‘œì ì¸ ë‰´ìŠ¤ ì„ íƒ
    - ì—°ê´€ë‰´ìŠ¤ ìƒì„±: ì¤‘ë³µì€ ì•„ë‹ˆì§€ë§Œ ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ë§¤ì¹­
    
    ì•Œê³ ë¦¬ì¦˜:
    - SBERT: í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© (ì˜ë¯¸ì  ìœ ì‚¬ë„)
    - TF-IDF: í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ (êµ¬ë¬¸ì  ìœ ì‚¬ë„)  
    - Cosine Similarity: ë²¡í„° ê°„ ê°ë„ ê¸°ë°˜ ìœ ì‚¬ë„ ì¸¡ì •
    - Union-Find: íš¨ìœ¨ì ì¸ ì§‘í•© ì—°ì‚°ìœ¼ë¡œ O(Î±(n)) ì‹œê°„ë³µì¡ë„
    """
    
    def __init__(self, fileserver_service):
        self.fileserver_service = fileserver_service
        self.sbert_model: Optional[SentenceTransformer] = None
        self.okt = Okt()  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
        
        # íŒŒì´ì¬ ì›ë³¸ê³¼ ë™ì¼í•œ ì„¤ì •
        self.THRESHOLD_TITLE = settings.THRESHOLD_TITLE
        self.THRESHOLD_CONTENT = settings.THRESHOLD_CONTENT
        self.THRESHOLD_RELATED_MIN = settings.THRESHOLD_RELATED_MIN
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_processing_time': 0.0,
            'start_time': time.time()
        }
        
        # ë¶ˆìš©ì–´ ë° ì¤‘ìš” í‚¤ì›Œë“œ (íŒŒì´ì¬ ì›ë³¸ê³¼ ë™ì¼)
        self.STOPWORDS = {
            "ê¸°ì", "ì†ë³´", "ë‹¨ë…", "í¬í† ", "ì˜ìƒ"
        }
        
        self.IMPORTANT_KEYWORDS = {
            # êµ­ê°€ ë° ì§€ì—­ (í•œì)
            "ä¸­", "ç¾", "æ—¥", "éŸ“", "åŒ—", "å—", "è‹±", "ç¨", "ä»", "éœ²", "å°",
            # ì§€ì •í•™Â·ë°©í–¥
            "æ±", "è¥¿", "äº", "æ­",
            # ì •ì¹˜Â·ì•ˆë³´
            "æ ¸", "è»", "ç¸½", "æ°‘", "é»¨", "æ³•", "è£",
            # ê²½ì œÂ·ì‚°ì—…
            "é‡‘", "éŠ€", "æ²¹", "é›»", "è»Š",
            # ë¡œë§ˆì ì•½ì–´
            "EU", "AI", "G7", "OECD",
            # êµ­ê°€ ë° ì§€ì •í•™ (í•œê¸€)
            "ë¯¸", "ì¤‘", "í•œ", "ë¶", "ë‚¨", "ì¼", "ë™", "ì„œ", "ì˜", "ë…", "ëŸ¬", "êµ­",
            # ì •ì¹˜ í‚¤ì›Œë“œ (í•œê¸€)
            "ë¯¼", "ì´", "ë²•", "í•µ",
            # ì„±ì”¨ (í•œê¸€)
            "ê¹€", "ì´", "ë°•", "ìµœ", "ì •", "ê°•", "ì¡°", "ìœ¤", "ì¥", "ì„",
            "í•œ", "ì˜¤", "ì„œ", "ì‹ ", "ê¶Œ", "í™©", "ì•ˆ", "ì†¡", "ë¥˜", "í™",
            # ì„±ì”¨ (í•œì)
            "é‡‘", "æ", "æœ´", "å´”", "é„­", "å§œ", "è¶™", "å°¹", "å¼µ", "æ—",
            "éŸ“", "å³", "å¾", "ç”³", "æ¬Š", "é»ƒ", "å®‰", "å®‹", "æŸ³", "æ´ª"
        }
    
    async def initialize(self):
        """SBERT ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info(f"ğŸ¤– SBERT ëª¨ë¸ ë¡œë”© ì‹œì‘: {settings.SBERT_MODEL_NAME}")
            
            # SBERT ëª¨ë¸ ë¡œë”© (ë¹„ë™ê¸° ì²˜ë¦¬)
            loop = asyncio.get_event_loop()
            self.sbert_model = await loop.run_in_executor(
                None,
                self._load_sbert_model
            )
            
            logger.info("âœ… SBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ SBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _load_sbert_model(self) -> SentenceTransformer:
        """SBERT ëª¨ë¸ ë™ê¸° ë¡œë”©"""
        model = SentenceTransformer(
            settings.SBERT_MODEL_NAME,
            device=settings.SBERT_DEVICE
        )
        return model
    
    def is_model_ready(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return self.sbert_model is not None
    
    async def run_deduplication(self, category: str, file_timestamp: str = None) -> DeduplicationResponse:
        """
        ì¤‘ë³µì œê±° ë©”ì¸ ë¡œì§
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # ì¹´í…Œê³ ë¦¬ ê°’ ì •ê·œí™” (Enum ê°ì²´ì¸ ê²½ìš° ê°’ ì¶”ì¶œ)
            original_category = category
            if hasattr(category, 'value'):
                category = category.value
            elif str(category).startswith('Category.'):
                category = str(category).replace('Category.', '')
            
            logger.info(f"ğŸ” ì¤‘ë³µì œê±° ì‹œì‘: {original_category} â†’ {category}")
            
            # 1. íŒŒì¼ì„œë²„ì—ì„œ ì›ë³¸ ë°ì´í„° ì¡°íšŒ
            logger.info(f"ğŸ” íŒŒì¼ì„œë²„ì—ì„œ detail ë°ì´í„° ì¡°íšŒ ì‹œì‘: {category}")
            original_news = self.fileserver_service.get_news_from_csv(category, "detail")
            logger.info(f"ğŸ” detail ì¡°íšŒ ê²°ê³¼: {len(original_news)}ê°œ")
            
            if not original_news:
                # listì—ì„œë„ ì‹œë„
                logger.info(f"ğŸ” íŒŒì¼ì„œë²„ì—ì„œ list ë°ì´í„° ì¡°íšŒ ì‹œë„: {category}")
                original_news = self.fileserver_service.get_news_from_csv(category, "list")
                logger.info(f"ğŸ” list ì¡°íšŒ ê²°ê³¼: {len(original_news)}ê°œ")
            
            if not original_news:
                logger.warning(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì—†ìŒ")
                return DeduplicationResponse(
                    category=category,
                    original_count=0,
                    deduplicated_count=0,
                    related_count=0,
                    removed_count=0,
                    processing_time_seconds=time.time() - start_time,
                    statistics={},
                    message="ì›ë³¸ ë°ì´í„° ì—†ìŒ"
                )
            
            # 2. ì œëª© ê¸°ë°˜ ìœ ì‚¬ ê·¸ë£¹ ìƒì„± (íŒŒì´ì¬ build_title_similarity_groups)
            title_groups = await self._build_title_similarity_groups(original_news)
            
            # 3. ë³¸ë¬¸ ê¸°ë°˜ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ë° ì—°ê´€ë‰´ìŠ¤ ìƒì„±
            deduplicated_news, related_news, removed_count = await self._process_content_deduplication(
                title_groups, original_news
            )
            
            # 4. íŒŒì¼ì„œë²„ì— ê²°ê³¼ ì €ì¥
            self.fileserver_service.save_news_to_csv(category, deduplicated_news, "deduplicated")
            self.fileserver_service.save_related_news_to_csv(category, related_news)
            
            # 5. í†µê³„ ê³„ì‚°
            processing_time = time.time() - start_time
            self.stats['successful_requests'] += 1
            self.stats['total_processing_time'] += processing_time
            
            statistics = {
                'original': len(original_news),
                'deduplicated': len(deduplicated_news),
                'related': len(related_news),
                'removed': removed_count,
                'removal_rate': removed_count / len(original_news) if original_news else 0,
                'title_groups': len(title_groups)
            }
            
            logger.info(f"âœ… {category} ì¤‘ë³µì œê±° ì™„ë£Œ: {len(original_news)}ê°œ â†’ {len(deduplicated_news)}ê°œ, "
                       f"ì—°ê´€ë‰´ìŠ¤ {len(related_news)}ê°œ, ì œê±° {removed_count}ê°œ")
            
            return DeduplicationResponse(
                category=category,
                original_count=len(original_news),
                deduplicated_count=len(deduplicated_news),
                related_count=len(related_news),
                removed_count=removed_count,
                processing_time_seconds=processing_time,
                statistics=statistics,
                message="ì¤‘ë³µì œê±° ì™„ë£Œ"
            )
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ {category} ì¤‘ë³µì œê±° ì‹¤íŒ¨: {e}")
            raise
    
    async def _build_title_similarity_groups(self, news_list: List[NewsDetail]) -> List[List[int]]:
        """
        ì œëª© ê¸°ë°˜ ìœ ì‚¬ ê·¸ë£¹ ìƒì„± - íŒŒì´ì¬ build_title_similarity_groupsì™€ ë™ì¼
        """
        # 1. ì œëª© ì „ì²˜ë¦¬ (íŒŒì´ì¬: df['clean_title'] = df['title'].apply(preprocess_titles))
        clean_titles = []
        for news in news_list:
            clean_title = await self._preprocess_titles(news.title)
            clean_titles.append(clean_title)
        
        # 2. ì œëª© ìœ ì‚¬ë„ ê³„ì‚° (íŒŒì´ì¬: compute_title_similarity)
        similar_pairs = await self._compute_title_similarity(clean_titles)
        
        # 3. Union-Findë¡œ ê·¸ë£¹ ìƒì„± (íŒŒì´ì¬: group_by_union_find)
        groups = self._group_by_union_find(similar_pairs, len(news_list))
        
        logger.info(f"ì œëª© ê·¸ë£¹í•‘ ì™„ë£Œ: {len(groups)}ê°œ ê·¸ë£¹ ìƒì„±")
        return groups
    
    async def _preprocess_titles(self, text: str) -> str:
        """
        ì œëª© ì „ì²˜ë¦¬ - ì›ë³¸ Python preprocessing_title.pyì™€ 100% ë™ì¼
        
        ì›ë³¸ Python ì½”ë“œ:
        def preprocess_titles(text):
            if pd.isna(text):
                return ''
            
            text = str(text)
            text = re.sub(r'[^\w\s]', ' ', text)         # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            text = re.sub(r'\d+', '', text)              # ìˆ«ì ì œê±°
            text = re.sub(r'\s+', ' ', text).strip()     # ê³µë°± ì •ë¦¬

            tokens = okt.nouns(text)                     # ëª…ì‚¬ ì¶”ì¶œ
            tokens = [
                t for t in tokens 
                if (len(t) > 1 or t in IMPORTANT_KEYWORDS) and t not in STOPWORDS
            ]
            return ' '.join(tokens)
        """
        # íŒŒì´ì¬: if pd.isna(text): return ''
        if not text or not isinstance(text, str) or text.strip() == '':
            return ""
        
        # íŒŒì´ì¬: text = str(text)
        text = str(text)
        
        # íŒŒì´ì¬: text = re.sub(r'[^\w\s]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # íŒŒì´ì¬: text = re.sub(r'\d+', '', text)  # ìˆ«ì ì œê±°
        text = re.sub(r'\d+', '', text)
        
        # íŒŒì´ì¬: text = re.sub(r'\s+', ' ', text).strip()  # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # íŒŒì´ì¬: tokens = okt.nouns(text)  # ëª…ì‚¬ ì¶”ì¶œ
        try:
            tokens = self.okt.nouns(text)
        except Exception as e:
            logger.warning(f"KoNLPy ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # fallback: ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
            tokens = text.split()
        
        # íŒŒì´ì¬: tokens = [t for t in tokens if (len(t) > 1 or t in IMPORTANT_KEYWORDS) and t not in STOPWORDS]
        filtered_tokens = [
            token for token in tokens
            if (len(token) > 1 or token in self.IMPORTANT_KEYWORDS) and token not in self.STOPWORDS
        ]
        
        # íŒŒì´ì¬: return ' '.join(tokens)
        return ' '.join(filtered_tokens)
    
    async def _preprocess_content(self, text: str) -> str:
        """
        ë³¸ë¬¸ ì „ì²˜ë¦¬ - íŒŒì´ì¬ preprocessing_content.pyì˜ preprocess_contentì™€ 100% ë™ì¼
        HTML íƒœê·¸ ì œê±° ê¸°ëŠ¥ ì¶”ê°€
        """
        # íŒŒì´ì¬: if not isinstance(text, str): return ''
        if not isinstance(text, str) or not text:
            return ""
        
        # HTML íŒŒì‹±ìœ¼ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë” ì •í™•í•˜ê³  ì•ˆì „í•¨)
        try:
            soup = BeautifulSoup(text, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
        except Exception as e:
            logger.warning(f"HTML íŒŒì‹± ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ fallback: {e}")
            # Fallback: ê¸°ì¡´ ì •ê·œì‹ ë°©ì‹
            text = re.sub(r'<[^>]+>', ' ', text)
            text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
        
        # íŒŒì´ì¬: text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # íŒŒì´ì¬: text = re.sub(r'\d+', '', text)
        text = re.sub(r'\d+', '', text)
        
        # íŒŒì´ì¬: text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r'\s+', ' ', text).strip()
        
        # íŒŒì´ì¬: tokens = [t for t in okt.morphs(text) if (len(t) > 1 or t in IMPORTANT_KEYWORDS) and t not in STOPWORDS]
        try:
            morphs = self.okt.morphs(text)
        except Exception:
            # fallback: ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
            morphs = text.split()
        
        filtered_tokens = [
            token for token in morphs
            if (len(token) > 1 or token in self.IMPORTANT_KEYWORDS) and token not in self.STOPWORDS
        ]
        
        # íŒŒì´ì¬: return ' '.join(tokens)
        return ' '.join(filtered_tokens)
    
    async def _compute_title_similarity(self, clean_titles: List[str]) -> List[Tuple[int, int, float]]:
        """
        ì œëª© ìœ ì‚¬ë„ ê³„ì‚° - íŒŒì´ì¬ grouping.pyì˜ compute_title_similarityì™€ ë™ì¼
        """
        if len(clean_titles) < 2:
            return []
        
        # TF-IDF ë²¡í„°í™” (íŒŒì´ì¬ê³¼ ë™ì¼)
        vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # ì´ë¯¸ ì „ì²˜ë¦¬ì—ì„œ ë¶ˆìš©ì–´ ì œê±°
            ngram_range=(1, 2)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(clean_titles)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # ì„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ ìŒ ì¶”ì¶œ
            similar_pairs = []
            for i in range(len(clean_titles)):
                for j in range(i + 1, len(clean_titles)):
                    if similarity_matrix[i][j] >= self.THRESHOLD_TITLE:
                        similar_pairs.append((i, j, similarity_matrix[i][j]))
            
            logger.debug(f"ì œëª© ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {len(similar_pairs)}ê°œ ìœ ì‚¬ ìŒ ë°œê²¬")
            return similar_pairs
            
        except Exception as e:
            logger.warning(f"TF-IDF ê³„ì‚° ì‹¤íŒ¨: {e}")
            return []
    
    def _group_by_union_find(self, similar_pairs: List[Tuple[int, int, float]], total_size: int) -> List[List[int]]:
        """
        Union-Find ê·¸ë£¹í•‘ - íŒŒì´ì¬ grouping.pyì˜ group_by_union_findì™€ 100% ë™ì¼
        """
        parent = {}
        
        def find(x):
            if x not in parent:
                parent[x] = x
            while parent[x] != x:
                parent[x] = parent[parent[x]]  # ê²½ë¡œ ì••ì¶•
                x = parent[x]
            return x
        
        def union(x, y):
            root_x = find(x)
            root_y = find(y)
            if root_x != root_y:
                parent[root_y] = root_x
        
        # Union ì—°ì‚° ìˆ˜í–‰
        for i, j, similarity in similar_pairs:
            union(i, j)
        
        # ê·¸ë£¹ ìƒì„±
        groups_dict = defaultdict(list)
        for node in parent.keys():
            root = find(node)
            groups_dict[root].append(node)
        
        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        groups = list(groups_dict.values())
        
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬ (í° ê·¸ë£¹ë¶€í„°)
        groups.sort(key=len, reverse=True)
        
        return groups
    
    async def _process_content_deduplication(
        self, 
        title_groups: List[List[int]], 
        original_news: List[NewsDetail]
    ) -> Tuple[List[NewsDetail], List[RelatedNewsPair], int]:
        """
        ë³¸ë¬¸ ê¸°ë°˜ ì¤‘ë³µì œê±° ë° ì—°ê´€ë‰´ìŠ¤ ìƒì„±
        """
        deduplicated_news = []
        all_related_news = []
        total_removed = 0
        processed_indices = set()
        
        for group in title_groups:
            if len(group) == 1:
                # ë‹¨ì¼ ê¸°ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                news = original_news[group[0]]

                news.dedup_state = "KEPT"
                deduplicated_news.append(news)
                processed_indices.add(group[0])
            else:
                # ê·¸ë£¹ ë‚´ ë³¸ë¬¸ ê¸°ë°˜ ì¤‘ë³µì œê±°
                result = await self._filter_and_pick_representative_by_content(group, original_news)
                
                if result['representative_index'] is not None:
                    # ëŒ€í‘œ ê¸°ì‚¬ (ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ì— ë”°ë¼ ì„¤ì •)
                    rep_news = original_news[result['representative_index']]

                    rep_news.dedup_state = "REPRESENTATIVE"
                    deduplicated_news.append(rep_news)
                    processed_indices.add(result['representative_index'])
                    
                    # ì—°ê´€ ê¸°ì‚¬ë“¤ (ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ì— ë”°ë¼ ì„¤ì •)
                    for idx in result['related_indices']:
                        related_news = original_news[idx]

                        related_news.dedup_state = "RELATED"
                        deduplicated_news.append(related_news)
                        processed_indices.add(idx)
                    
                    # ì—°ê´€ë‰´ìŠ¤ ìŒ ì¶”ê°€
                    all_related_news.extend(result['related_pairs'])
                    
                    # ì œê±°ëœ ê¸°ì‚¬ ìˆ˜ ì¶”ê°€
                    total_removed += len(result['removed_indices'])
                    processed_indices.update(result['removed_indices'])
                else:
                    # ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ì‹¤íŒ¨ ì‹œ ëª¨ë“  ê¸°ì‚¬ ìœ ì§€
                    for idx in group:
                        news = original_news[idx]
                        news.dedup_state = "KEPT"
                        deduplicated_news.append(news)
                        processed_indices.add(idx)
        
        # ê·¸ë£¹í™”ë˜ì§€ ì•Šì€ ë…ë¦½ ê¸°ì‚¬ë“¤ ì¶”ê°€
        for i in range(len(original_news)):
            if i not in processed_indices:
                news = original_news[i]

                news.dedup_state = "KEPT"
                deduplicated_news.append(news)
        
        return deduplicated_news, all_related_news, total_removed
    
    async def _filter_and_pick_representative_by_content(
        self, 
        group: List[int], 
        news_list: List[NewsDetail]
    ) -> Dict[str, Any]:
        """
        ë³¸ë¬¸ ê¸°ë°˜ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • 
        """
        if len(group) == 1:
            return {
                'representative_index': group[0],
                'removed_indices': [],
                'related_indices': [],
                'related_pairs': []
            }
        
        # íŒŒì´ì¬: docs = [preprocess_content(df.loc[i, 'content']) for i in indices]
        docs = []
        for idx in group:
            content = news_list[idx].content or ""
            preprocessed = await self._preprocess_content(content)
            docs.append(preprocessed)
        
        # â­ íŒŒì´ì¬ê³¼ 100% ë™ì¼: SBERT ì‚¬ìš©
        # íŒŒì´ì¬: embeddings = model.encode(docs, convert_to_tensor=True)
        # íŒŒì´ì¬: sim_matrix = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()
        try:
            embeddings = self.sbert_model.encode(docs, convert_to_tensor=True)
            sim_matrix = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()
        except Exception as e:
            logger.warning(f"SBERT ì²˜ë¦¬ ì‹¤íŒ¨, TF-IDFë¡œ fallback: {e}")
            # Fallback: TF-IDF ì‚¬ìš©
            sim_matrix = await self._compute_content_similarity_tfidf(docs)
        
        # íŒŒì´ì¬: row_avg = sim_matrix.mean(axis=1)
        # íŒŒì´ì¬: rep_idx = indices[int(row_avg.argmax())]
        row_avg = np.mean(sim_matrix, axis=1)
        max_avg_idx = int(np.argmax(row_avg))
        rep_idx = group[max_avg_idx]
        
        removed_indices = []
        related_indices = []
        related_pairs = []
        
        # íŒŒì´ì¬ ë¡œì§ê³¼ 100% ë™ì¼í•œ ë¶„ë¥˜
        for i, idx in enumerate(group):
            if idx == rep_idx:
                continue
            
            # íŒŒì´ì¬: sim = sim_matrix[i][group.index(rep_idx)]
            sim = sim_matrix[i][max_avg_idx]
            
            if sim >= self.THRESHOLD_CONTENT:
                # ì¤‘ë³µ ì œê±°
                removed_indices.append(idx)
            elif sim >= self.THRESHOLD_RELATED_MIN:
                # ì—°ê´€ë‰´ìŠ¤ë¡œ ë¶„ë¥˜
                related_indices.append(idx)
                
                # ì—°ê´€ë‰´ìŠ¤ ìŒ ìƒì„±
                rep_oid_aid = self._generate_oid_aid(news_list[rep_idx])
                related_oid_aid = self._generate_oid_aid(news_list[idx])
                
                related_pairs.append(RelatedNewsPair(
                    rep_oid_aid=rep_oid_aid,
                    related_oid_aid=related_oid_aid,
                    similarity=round(float(sim), 4)
                ))
        
        return {
            'representative_index': rep_idx,
            'removed_indices': removed_indices,
            'related_indices': related_indices,
            'related_pairs': related_pairs
        }
    
    async def _compute_content_similarity_tfidf(self, docs: List[str]) -> np.ndarray:
        """TF-IDF ê¸°ë°˜ ë³¸ë¬¸ ìœ ì‚¬ë„ ê³„ì‚° (SBERT fallback)"""
        if not docs or len(docs) < 2:
            return np.eye(len(docs))
        
        try:
            vectorizer = TfidfVectorizer(max_features=1000)
            tfidf_matrix = vectorizer.fit_transform(docs)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            return similarity_matrix
        except Exception:
            # ìµœì¢… fallback: ë‹¨ìœ„í–‰ë ¬
            return np.eye(len(docs))
    
    def _generate_oid_aid(self, news: NewsDetail) -> str:
        """OID-AID ìƒì„± (Java ë¡œì§ê³¼ ë™ì¼)"""
        if news.oid_aid:
            return news.oid_aid
        
        url = news.link
        if url and "/article/" in url:
            try:
                parts = url.split("/article/")[1].split("/")
                if len(parts) >= 2:
                    return f"{parts[0]}-{parts[1]}"
            except Exception:
                pass
        
        return f"extracted_{abs(hash(url))}"
    
    async def get_deduplicated_news(self, category: str) -> List[NewsDetail]:
        """ì¤‘ë³µì œê±°ëœ ë‰´ìŠ¤ ì¡°íšŒ"""
        return self.fileserver_service.get_news_from_csv(category, "deduplicated")
    
    async def get_related_news(self, category: str) -> List[RelatedNewsPair]:
        """ì—°ê´€ë‰´ìŠ¤ ì¡°íšŒ"""
        # íŒŒì¼ì„œë²„ì—ì„œ ì—°ê´€ë‰´ìŠ¤ ì¡°íšŒ (êµ¬í˜„ í•„ìš”ì‹œ)
        return []
    
    async def get_stats(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ"""
        uptime = time.time() - self.stats['start_time']
        avg_processing_time = (
            self.stats['total_processing_time'] / self.stats['successful_requests']
            if self.stats['successful_requests'] > 0 else 0
        )
        
        return {
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'failed_requests': self.stats['failed_requests'],
            'average_processing_time': avg_processing_time,
            'active_requests': 0,  # í˜„ì¬ í™œì„± ìš”ì²­ ìˆ˜ (êµ¬í˜„ í•„ìš”ì‹œ)
            'model_loaded': self.is_model_ready(),
            'uptime_seconds': uptime
        }
